{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfa6080-c0d9-4f91-84a7-b0ece9a2362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zengs/data/anaconda3/envs/venv_pl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: torch.Size([4, 1])\n",
      "train_obj shape: torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zengs/data/anaconda3/envs/venv_pl/lib/python3.8/site-packages/botorch/models/utils/assorted.py:201: InputDataWarning: Input data is not standardized. Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "/home/zengs/data/anaconda3/envs/venv_pl/lib/python3.8/site-packages/botorch/models/utils/assorted.py:201: InputDataWarning: Input data is not standardized. Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  1: Hypervolume qNEHVI = 0.74, time = 9.47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zengs/data/anaconda3/envs/venv_pl/lib/python3.8/site-packages/botorch/models/utils/assorted.py:201: InputDataWarning: Input data is not standardized. Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  2: Hypervolume qNEHVI = 0.75, time = 4.88.\n",
      "\n",
      "Batch  3: Hypervolume qNEHVI = 0.96, time = 5.42.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zengs/data/anaconda3/envs/venv_pl/lib/python3.8/site-packages/botorch/models/utils/assorted.py:201: InputDataWarning: Input data is not standardized. Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from botorch.models.gp_regression import FixedNoiseGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qExpectedHypervolumeImprovement,\n",
    "    qNoisyExpectedHypervolumeImprovement,\n",
    ")\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qNoisyExpectedHypervolumeImprovement\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import FastNondominatedPartitioning\n",
    "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from botorch.optim.optimize import optimize_acqf_list\n",
    "import torch\n",
    "import time\n",
    "import warnings\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import DominatedPartitioning\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
    "\n",
    "# Define global variables (assuming tkwargs and NOISE_SE are already defined)\n",
    "BATCH_SIZE = 4\n",
    "NUM_RESTARTS = 10  # Adjust as needed\n",
    "RAW_SAMPLES = 512  # Adjust as needed\n",
    "ref_point = torch.tensor([-0.5, -0.2], **tkwargs)  # Reference point for qNEHVI\n",
    "bounds = torch.stack([torch.zeros(1, **tkwargs), torch.ones(1, **tkwargs)])\n",
    "standard_bounds = torch.zeros(2, 1, **tkwargs)\n",
    "standard_bounds[1] = 1\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def modified_sine_function_torch(x):\n",
    "    return torch.sin(5 * torch.pi * x) * (1 - x) ** 2\n",
    "\n",
    "\n",
    "def polynomial_exponential_function_torch(x):\n",
    "    return x * torch.exp(-x ** 2) + 0.5 * torch.sin(4 * torch.pi * x)\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "NOISE_SE = torch.tensor([15.19, 0.63], **tkwargs)\n",
    "\n",
    "\n",
    "def generate_initial_data(n=6):\n",
    "    train_x = draw_sobol_samples(bounds=bounds, n=n, q=1).squeeze(1)\n",
    "    train_obj1_true = modified_sine_function_torch(train_x)\n",
    "    train_obj2_true = polynomial_exponential_function_torch(train_x)\n",
    "\n",
    "    # Stack along the second dimension to match the shape of train_x\n",
    "    train_obj_true = torch.stack([train_obj1_true, train_obj2_true], dim=-1).squeeze(-2)\n",
    "    train_obj = train_obj_true + torch.randn_like(train_obj_true) * NOISE_SE\n",
    "    return train_x, train_obj, train_obj_true\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def initialize_model(train_x, train_obj, noise_se):\n",
    "    models = []\n",
    "    for i in range(train_obj.shape[-1]):  # Loop over each objective\n",
    "        train_y = train_obj[..., i:i + 1]\n",
    "        train_yvar = torch.full_like(train_y, noise_se[i] ** 2)\n",
    "\n",
    "        # Initialize the GP model\n",
    "        model = FixedNoiseGP(train_x, train_y, train_yvar)\n",
    "        models.append(model)\n",
    "\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "BATCH_SIZE = 4\n",
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
    "\n",
    "bounds = torch.stack([torch.zeros(1, **tkwargs), torch.ones(1, **tkwargs)])\n",
    "\n",
    "\n",
    "def optimize_qehvi_and_get_observation(model, train_x, sampler, bounds):\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, bounds)).mean\n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point=ref_point,\n",
    "        Y=pred,\n",
    "    )\n",
    "\n",
    "    bounds = torch.stack([torch.zeros(1, **tkwargs), torch.ones(1, **tkwargs)])\n",
    "\n",
    "    acq_func = qExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=ref_point,\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "\n",
    "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
    "    new_obj_true = torch.stack([\n",
    "        modified_sine_function_torch(new_x),\n",
    "        polynomial_exponential_function_torch(new_x)\n",
    "    ], -1)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def optimize_qnehvi_and_get_observation(model, train_x, sampler):\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, bounds)).mean\n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point=ref_point,\n",
    "        Y=pred,\n",
    "    )\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=ref_point.tolist(),\n",
    "        X_baseline=normalize(train_x, bounds),\n",
    "        prune_baseline=True,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
    "    new_obj_true = torch.stack([\n",
    "        modified_sine_function_torch(new_x),\n",
    "        polynomial_exponential_function_torch(new_x)\n",
    "    ], -1)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def optimize_qnparego_and_get_observation(model, train_x, sampler):\n",
    "\n",
    "    train_x_normalized = normalize(train_x, bounds)\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(train_x_normalized).mean\n",
    "    acq_func_list = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        weights = sample_simplex(2, **tkwargs).squeeze()  # 2 for two objectives\n",
    "        objective = GenericMCObjective(\n",
    "            lambda Y: get_chebyshev_scalarization(weights=weights, Y=Y)\n",
    "        )\n",
    "        acq_func = qNoisyExpectedImprovement(\n",
    "            model=model,\n",
    "            objective=objective,\n",
    "            X_baseline=train_x_normalized,\n",
    "            sampler=sampler,\n",
    "            prune_baseline=True,\n",
    "        )\n",
    "        acq_func_list.append(acq_func)\n",
    "\n",
    "    candidates, _ = optimize_acqf_list(\n",
    "        acq_function_list=acq_func_list,\n",
    "        bounds=standard_bounds,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "    )\n",
    "\n",
    "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
    "    new_obj_true = torch.stack([\n",
    "        modified_sine_function_torch(new_x),\n",
    "        polynomial_exponential_function_torch(new_x)\n",
    "    ], -1)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "N_BATCH = 3  # Adjust as needed\n",
    "MC_SAMPLES = 128  # Adjust as needed\n",
    "verbose = True\n",
    "\n",
    "# Initialize lists to store hypervolume values\n",
    "hvs_qparego, hvs_qehvi, hvs_qnehvi, hvs_random = [], [], [], []\n",
    "\n",
    "# Generate initial training data and initialize model\n",
    "train_x, train_obj, train_obj_true = generate_initial_data(n=2 * 1 + 2)  # Adjust as needed\n",
    "print(\"train_x shape:\", train_x.shape)\n",
    "print(\"train_obj shape:\", train_obj.shape)\n",
    "mll, model = initialize_model(train_x, train_obj, NOISE_SE)\n",
    "\n",
    "# Compute initial hypervolume\n",
    "bd = DominatedPartitioning(ref_point=ref_point, Y=train_obj_true)\n",
    "initial_volume = bd.compute_hypervolume().item()\n",
    "hvs_qparego.append(initial_volume)\n",
    "hvs_qehvi.append(initial_volume)\n",
    "hvs_qnehvi.append(initial_volume)\n",
    "hvs_random.append(initial_volume)\n",
    "\n",
    "# Define samplers for acquisition functions\n",
    "qparego_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "qehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "qnehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "\n",
    "# Bayesian Optimization Loop\n",
    "for iteration in range(1, N_BATCH + 1):\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    # Fit the models\n",
    "    fit_gpytorch_mll(mll)\n",
    "\n",
    "    # Optimize acquisition functions and get new observations\n",
    "    new_x, new_obj_temp, new_obj_true_temp = optimize_qehvi_and_get_observation(\n",
    "        model, train_x, qehvi_sampler, bounds  # Use the appropriate function and sampler\n",
    "    )\n",
    "\n",
    "    new_obj = new_obj_temp.view(-1, 2)\n",
    "\n",
    "    new_obj_true = new_obj_true_temp.view(-1, 2)\n",
    "\n",
    "    # Update training points\n",
    "    train_x = torch.cat([train_x, new_x])\n",
    "    train_obj = torch.cat([train_obj, new_obj])\n",
    "    train_obj_true = torch.cat([train_obj_true, new_obj_true])\n",
    "\n",
    "    # Reinitialize the model\n",
    "    mll, model = initialize_model(train_x, train_obj, NOISE_SE)\n",
    "\n",
    "    # Update hypervolume\n",
    "    bd = DominatedPartitioning(ref_point=ref_point, Y=train_obj_true)\n",
    "    volume = bd.compute_hypervolume().item()\n",
    "    hvs_qnehvi.append(volume)  # Update the correct hypervolume list\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "    if verbose:\n",
    "        print(f\"\\nBatch {iteration:>2}: Hypervolume qNEHVI = {volume:>4.2f}, time = {t1 - t0:>4.2f}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d02c3ab-2562-4cde-86eb-831b4d18a199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6539858692428603,\n",
       " 0.7382729687934243,\n",
       " 0.7486096920491598,\n",
       " 0.9590217860130419]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvs_qnehvi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
